import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import copy
import time
from torch.autograd  import Variable
#import seaborn

class EncoderDecoder(nn.Module):
    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator
    def forward(self, src, tgt, src_mask , tgt_mask):
        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)
    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)
    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)

class Generator(nn.Module):
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.linear(d_model, vocab)
    def forward(self, x):
        #log_softmax 就是对softmax 取log，
        #参考：https://www.pythonf.cn/read/63846
        #dim = -1 输入如果是 batch * row * col，  就是对每一行的col个值进行softmax
        return F.log_softmax(self.proj(x), dim = -1)


def clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])

class Encoder(nn.Module):
    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)
    def forward(self, x, mask):
        """
        实现 N * layer的前向，并对最后的输出进行norm，
        每一层的layer由EncoderLayer 来进行实现
        :param x:
        :param mask:
        :return:
        """
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
    
class LayerNorm(nn.Module):
    """
    实现layerNorm
    """
    def __init__(self, features, eps = 1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps
    def forward(self, x):
        mean = x.mean(-1, keepdim = True)
        std = x.std(-1, keepdim = True)
        return self.a_2 * (x-mean) / (std + self.eps) + self.b_2

class SublayerConnection(nn.Module):
    """
    residual connection followed by a layer norm
    """
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout == nn.Dropout(dropout)
        
    def froward(self, x, sublayer):
        return x + self.dropout(sublayer(self.norm(x)))

class EncoderLayer(nn.Module):
    """
    self_attn and feed forward
    这里不同于pytorch 官方代码中的实现，在pytorch实现中  最后是 
        layernorm(x + dropout( attention(x)) 
        layernorm(x + dropout( feedforward(x))
    而这里是：
        x + dropout(attention(layernorm(x)))
        x + dropout(feedforward(layernorm(x)))
    """
    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size
    def forward(self, x, mask):
        # self attn
        x = self.sublayer[0](x , lambda x :self.self_attn(x, x, x, mask))
        #feed froward
        return self.sublayer[1](x, self.feed_forward)


class Decoder(nn.Module):
    """
    generic N layer decoder with masking
    """
    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)
    def forward(self, x, memory, src_mask, tgt_mask):
        #这里和encoder不同的地方是，这里还是使用了encoder的最后的输出
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)

class DecoderLayer(nn.Module):
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3)
    def forward(self, x, memory, src_mask, tgt_mask):
        #在这里，encoder最后的输出会作为memory，也就是这个attention的key和value
        m = memory
        x = self.sublayer[0](x, lambda x : self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, lambda x : self.src_attn(x, m, m, src_mask))
        return self.sublayer[2](x, self.feed_forward)

def subsequent_mask(size):
    """
    mask out subsequent positions for decoder
    :param size:
    :return:
    """
    attn_shape = (1, size, size)
    #np.triu 是上三角矩阵
    subsequent_mask = np.triu(np.ones(attn_shape), k = 1).astype('unit8')
    #返回结果表示的是，每个目标词（行）查看的位置（列）。在训练期间，当前解码位置的词不能attn到后续位置的词
    return torch.from_numpy(subsequent_mask) == 0

#########attenthon
"""

attention(Q, K, V) = softmax( Q * transport(K) / sqrt(dk) )  V

"""


def attention(query, key, value, mask = None, dropout = None):
    """
    compote scaled dot product attention
    """
    d_k = query.size(-1)   
    score = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(d_k)
    if mask is not None:
        ###传入一个mask，遮盖掉那些不应该存在的值，（decoder中，要先遮住的值）
        score = score.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(score, dim= -1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn


class MultiHeadedAttention(nn.Module):
    """
    原始公式是这样：
    MultiHead(Q,K,V) = concat(head1,......,headn)WO
    其中，headi = Attention(Q* WiQ, K*WiK, V*WiV)
    参数WiQ，WiK，WiV都是 d_model * dk的
    
    这里的实现，直接一步到位
    
    """
    def __init__(self, h, d_model, dropout = 0.1):
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        self.d_k = d_model // h
        self.h  = h
        #linear is  y = x * transpose(w) + b
        ###that is input batch,d_model,  output batch,d_model
        self.linear = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p = dropout)
    def forward(self, query, key, value, mask = None):
        if mask is not None:
            mask= mask.unsqueeze(1)
        nbatches = query.size(0)
        # do all the linear projections in batch from d_model -> h x d_k
        
        #zip(self.linear, (query,key,value)) => linear[0],query   linear[1],key, linear[2],value
        # 这个linear里面，每一个w是d_model * d_model,拆分来看， 可以认为是 h个 d_model * d_k，对应h个头
        #把linear的结果进行拆分成  h个 就是了【view 后然后 transpose】
        
        #得到的query 是 batch * h * length * d_k
        
        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)
                             for l, x in zip(self.linear, (query, key, value))]
        
        x, self.attn = attention(query, key, value, mask = mask, dropout=self.dropout)
        #乘完后，然后再给恢复回去
        x = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)
        return self.linear[-1](x)
    

class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))


class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model
    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)

class PositionalEmcoding(nn.Module):
    def __init__(self, d_model, dropout, max_len = 5000):
        super(PositionalEmcoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0,d_model, 2)* -(math.log(10000.0)/d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    def forward(self, x):
        x = x + Variable(self.pe[:,:x.size(1)],requires_grad = False)
        return self.dropout(x)
        
def make_model(src_vocab, tgt_vocab, N = 6, d_model = 512, d_ff = 2048, h = 8, dropout = 0.1):
    c = copy.deepcopy
    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEmcoding(d_model, dropout)
    model = EncoderDecoder(
        Encoder(
            EncoderLayer(d_model,c(attn),c(ff),dropout),N,
            ),
        Decoder(
            DecoderLayer(d_model,c(attn), c(ff),dropout),N,  
            ),
        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
        nn.Sequential(Embeddings(d_model, tgt_vocab),c(position)),
        Generator(d_model, tgt_vocab)
    )
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform(p)
    return model

tmp_model = make_model(10,10,2)
